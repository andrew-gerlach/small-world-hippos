{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "load_chp.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZDFPnQ07MmEd",
        "qQzCA99sMryW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc-autonumbering": true,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew-gerlach/small-world-hippos/blob/bf-small-world-hippos/load_chp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIvhc_R1D6k1"
      },
      "source": [
        "# Human Connectome Project (HCP) Dataset loader\n",
        "\n",
        "The HCP dataset comprises resting-state and task-based fMRI from a large sample of human subjects. The NMA-curated dataset includes time series data that has been preprocessed and spatially-downsampled by aggregating within 360 regions of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bd3Lmr_Ujzs"
      },
      "source": [
        "#@title Warning: Not for use on Colab\n",
        "#@markdown Running this cell on Colab will mess up Colab. Do not run on Colab. This is meant for Peter's lab cluster\n",
        "\n",
        "import platform\n",
        "import re\n",
        "if re.search(\"^gra\\d+$\", platform.node()):\n",
        "  GRAHAM = True\n",
        "else:\n",
        "  GRAHAM = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXIw61Dk-M5E",
        "nbdime-conflicts": {
          "local_diff": [
            {
              "diff": [
                {
                  "key": "height",
                  "op": "remove"
                }
              ],
              "key": "colab",
              "op": "patch"
            },
            {
              "diff": [
                {
                  "key": 0,
                  "op": "addrange",
                  "valuelist": [
                    "d3988293-09bc-4f63-c296-3fbc2038f7bc"
                  ]
                },
                {
                  "key": 0,
                  "length": 1,
                  "op": "removerange"
                }
              ],
              "key": "outputId",
              "op": "patch"
            }
          ],
          "remote_diff": [
            {
              "key": "colab",
              "op": "remove"
            },
            {
              "key": "outputId",
              "op": "remove"
            }
          ]
        }
      },
      "source": [
        "#@title Module Loading\n",
        "\n",
        "\n",
        "# Check if GPU is available\n",
        "from tensorflow.python.client import device_lib\n",
        "runtimes = device_lib.list_local_devices()\n",
        "\n",
        "# Data modules\n",
        "import numpy as np\n",
        "if len(runtimes) > 1:\n",
        "  import cupy as cp\n",
        "else:\n",
        "  cp = None\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Plotting modules\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Graph theory modules\n",
        "import networkx as nx\n",
        "\n",
        "# Stats modules\n",
        "import scipy as sc\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# python modules\n",
        "import functools as func\n",
        "import operator as op\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import multiprocessing as mult\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "4eJOYQqgSMKV"
      },
      "source": [
        "#@title Figure settings\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "if GRAHAM:\n",
        "  plt.style.use(\".localdata/nma.mplstyle\")\n",
        "else:\n",
        "  plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSdEhS5jKzkb"
      },
      "source": [
        "# Basic parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL4crLoCzkzk"
      },
      "source": [
        "#@title Run to load basic parameters\n",
        "# The download cells will store the data in nested directories starting here:\n",
        "if GRAHAM:\n",
        "  HCP_DIR = \"./.localdata/hcp\"\n",
        "else:\n",
        "  HCP_DIR = \"./hcp\"\n",
        "if not os.path.isdir(HCP_DIR):\n",
        "  os.mkdir(HCP_DIR)\n",
        "\n",
        "# The data shared for NMA projects is a subset of the full HCP dataset\n",
        "N_SUBJECTS = 339\n",
        "# Flag for keeping tack if subjects have been adjusted\n",
        "subj_adjust = False\n",
        "\n",
        "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
        "N_PARCELS = 360\n",
        "\n",
        "# The acquisition parameters for all tasks were identical\n",
        "TR = 0.72  # Time resolution, in sec\n",
        "\n",
        "# The parcels are matched across hemispheres with the same order\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "\n",
        "# Each experiment was repeated multiple times in each subject\n",
        "N_RUNS_REST = 4\n",
        "N_RUNS_TASK = 2\n",
        "\n",
        "#NBACK number of frames per run\n",
        "FRAMES_PER_RUN = 405\n",
        "\n",
        "# Time series data are organized by experiment, with each experiment\n",
        "# having an LR and RL (phase-encode direction) acquistion\n",
        "BOLD_NAMES = [\n",
        "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
        "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
        "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
        "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
        "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
        "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
        "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
        "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
        "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
        "]\n",
        "\n",
        "# You may want to limit the subjects used during code development.\n",
        "# This will use all subjects:\n",
        "subjects = range(N_SUBJECTS)\n",
        "\n",
        "NODES_OF_INTEREST = [\"R_p9-46v\", \"R_IP2\", \"R_7Pm\", \"R_AVI\", \"L_a9-46v\", \"L_46\", \"L_AIP\", \"L_MI\"]\n",
        "NETWORKS_OF_INTEREST = networks = ['Cingulo-Oper', 'Default', 'Dorsal-atten', 'Frontopariet']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErP_ocaxK9FU"
      },
      "source": [
        "# Downloading data\n",
        "\n",
        "The rest and task data are shared in different files, but they will unpack into the same directory structure.\n",
        "\n",
        "Each file is fairly large and will take some time to download. If you are focusing only on rest or task analyses, you may not want to download only that dataset.\n",
        "\n",
        "We also separately provide some potentially useful behavioral covariate information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5Lh1SEyZ_Kdh"
      },
      "source": [
        "#@title Get Rest Data\n",
        "if not GRAHAM:\n",
        "  fname = \"hcp_rest.tgz\"\n",
        "  if not os.path.exists(fname):\n",
        "    !wget -qO $fname https://osf.io/bqp7m/download/\n",
        "    !tar -xzf $fname -C $HCP_DIR --strip-components=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5ubuyGN7siwI"
      },
      "source": [
        "#@title Get task data, covariates, and atlas\n",
        "if not GRAHAM:\n",
        "  fname = \"hcp_task.tgz\"\n",
        "  if not os.path.exists(fname):\n",
        "    !wget -qO $fname https://osf.io/s4h8j/download/\n",
        "    !tar -xzf $fname -C $HCP_DIR --strip-components=1\n",
        "\n",
        "  fname = \"hcp_covariates.tgz\"\n",
        "  if not os.path.exists(fname):\n",
        "    !wget -qO $fname https://osf.io/x5p4g/download/\n",
        "    !tar -xzf $fname -C $HCP_DIR --strip-components=1\n",
        "\n",
        "  fname = f\"{HCP_DIR}/atlas.npz\"\n",
        "  if not os.path.exists(fname):\n",
        "    !wget -qO $fname https://osf.io/j5kuc/download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaF8v-UBTcxq"
      },
      "source": [
        "## Loading region information\n",
        "\n",
        "Downloading either dataset will create the `regions.npy` file, which contains the region name and network assignment for each parcel.\n",
        "\n",
        "Detailed information about the name used for each region is provided [in the Supplement](https://static-content.springer.com/esm/art%3A10.1038%2Fnature18933/MediaObjects/41586_2016_BFnature18933_MOESM330_ESM.pdf) to [Glasser et al. 2016](https://www.nature.com/articles/nature18933).\n",
        "\n",
        "Information about the network parcellation is provided in [Ji et al, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6289683/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5_wD8BMbSwz"
      },
      "source": [
        "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
        "region_info = dict(\n",
        "    name=regions[0].tolist(),\n",
        "    network=regions[1],\n",
        "    myelin=regions[2].astype(np.float),)\n",
        "\n",
        "if not GRAHAM:\n",
        "  # Load github directory for access to Glasser atlas file\n",
        "  !git clone https://github.com/andrew-gerlach/small-world-hippos.git\n",
        "  atlas_name = 'small-world-hippos/MMP_in_MNI_corr.nii'\n",
        "else:\n",
        "  atlas_name = 'MMP_in_MNI_corr.nii'\n",
        "\n",
        "# Load Glasser atlas\n",
        "glasser_atlas = nib.load(atlas_name)\n",
        "img = glasser_atlas.get_fdata()\n",
        "# Renumber to be continuous from 1 to 360 (right is 1-180, left is 201-380 in original atlas)\n",
        "for i in range(201,381):\n",
        "  img[img == i] = i-20\n",
        "  \n",
        "# Initialize storage for list of nodes in each network\n",
        "unique_networks = np.unique(region_info['network'])\n",
        "nNet = len(unique_networks)          # number of networks\n",
        "network_regions = {}                 # initialize\n",
        "for net in unique_networks:\n",
        "  network_regions[net] = []\n",
        "\n",
        "# Populate lists and extract node volume\n",
        "nodeVol = np.zeros(N_PARCELS)\n",
        "for i in range(N_PARCELS):\n",
        "  nodeVol[i] = np.sum(img == (i+1))\n",
        "  network_regions[region_info['network'][i]].append(region_info['name'][i])\n",
        "\n",
        "with np.load(f\"{HCP_DIR}/atlas.npz\") as dobj:\n",
        "  atlas = dict(**dobj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUmm7gjhNBVF"
      },
      "source": [
        "We also provide the [parcellation on the fsaverage5 surface](https://figshare.com/articles/HCP-MMP1_0_projected_on_fsaverage/3498446) and approximate MNI coordinates of each region, which can be useful for visualization:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYDxvWrbIxxk"
      },
      "source": [
        "# Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gf0UQI9UfiZB"
      },
      "source": [
        "#@title GPU Utils\n",
        "#@markdown Run this to prevent errors in functions that use GPU\n",
        "\n",
        "def get_array_mod(arr):\n",
        "  if cp:\n",
        "    return cp.get_array_module(arr)\n",
        "  else:\n",
        "    return np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnq7H5h_IxLi"
      },
      "source": [
        "#@title Data Loading\n",
        "#@markdown Run to get functions related to data loading:\n",
        "#@markdown\n",
        "#@markdown ```\n",
        "#@markdown load_timeseries(subject, name, runs=None, concat=True, remove_mean=True)\n",
        "#@markdown load_task(name, mod=np)\n",
        "#@markdown ```\n",
        "def get_image_ids(name):\n",
        "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
        "\n",
        "    Args:\n",
        "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
        "    Returns:\n",
        "      run_ids (list of int) : Numeric ID for experiment image files\n",
        "\n",
        "  \"\"\"\n",
        "  run_ids = [\n",
        "    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n",
        "  ]\n",
        "  if not run_ids:\n",
        "    raise ValueError(f\"Found no data for '{name}''\")\n",
        "  return run_ids\n",
        "\n",
        "def load_timeseries(subject, name, runs=None, concat=True, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject.\n",
        "  \n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    name (str) : Name of experiment (\"rest\" or name of task) to load\n",
        "    run (None or int or list of ints): 0-based run(s) of the task to load,\n",
        "      or None to load all runs.\n",
        "    concat (bool) : If True, concatenate multiple runs in time\n",
        "    remove_mean (bool) : If True, subtract the parcel-wise mean\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_tp array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  # Get the list relative 0-based index of runs to use\n",
        "  if runs is None:\n",
        "    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n",
        "  elif isinstance(runs, int):\n",
        "    runs = [runs]\n",
        "\n",
        "  # Get the first (1-based) run id for this experiment \n",
        "  offset = get_image_ids(name)[0]\n",
        "\n",
        "  # Load each run's data\n",
        "  bold_data = [\n",
        "      load_single_timeseries(subject, offset + run, remove_mean) for run in runs\n",
        "  ]\n",
        "\n",
        "  # Optionally concatenate in time\n",
        "  if concat:\n",
        "    bold_data = np.concatenate(bold_data, axis=-1)\n",
        "\n",
        "  return bold_data\n",
        "\n",
        "\n",
        "def load_single_timeseries(subject, bold_run, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject and single run.\n",
        "  \n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    bold_run (int): 1-based run index, across all tasks\n",
        "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n",
        "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
        "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
        "  if remove_mean:\n",
        "    ts -= ts.mean(axis=1, keepdims=True)\n",
        "  return ts\n",
        "\n",
        "def load_evs(subject, name, condition):\n",
        "  \"\"\"Load EV (explanatory variable) data for one task condition.\n",
        "\n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    name (str) : Name of task\n",
        "    condition (str) : Name of condition\n",
        "\n",
        "  Returns\n",
        "    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
        "      of the condition for each run.\n",
        "\n",
        "  \"\"\"\n",
        "  evs = []\n",
        "  for id in get_image_ids(name):\n",
        "    task_key = BOLD_NAMES[id - 1]\n",
        "    ev_file = f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n",
        "    ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
        "    ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
        "    evs.append(ev)\n",
        "  return evs\n",
        "\n",
        "def load_task(name, mod=np):\n",
        "  \"\"\"\n",
        "  Load all timeseries for a given condition as a 3D numpy array:\n",
        "    (n_subjects x n_parcels x n_timepoints)\n",
        "  Possible conditions include \"rest\", \"wm\", etc. Can return results as either\n",
        "  a numpy or cupy array\n",
        "\n",
        "  Args:\n",
        "    name (str) : Condition to \n",
        "    mod (numpy/cupy module) : Module to use for forming the array\n",
        "  \n",
        "  Returns:\n",
        "    ts (sub x parcels x timepoints) : Array of BOLD datapoints\n",
        "  \"\"\"\n",
        "  assert mod is np or mod is cp, \"Use either numpy (np) or cupy (cp) as mod\"\n",
        "  if mod is cp and cp:\n",
        "    return cp.array([cp.asarray(load_timeseries(subject, name)) for subject in subjects])\n",
        "  return mod.array([load_timeseries(subject, name) for subject in subjects])\n",
        "\n",
        "\n",
        "task = \"wm\"\n",
        "conds = {'0bk': [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\"],\n",
        "         '2bk': [\"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\"]}\n",
        "\n",
        "N_CONDS = len(conds['2bk'])\n",
        "BLOCK_FRAMES = int(27.5/TR) # 27.5s is the duration of each block\n",
        "\n",
        "def load_wm_data(mod=np, drop=[]):\n",
        "  # intialize\n",
        "  timeseries_task_wm = {}\n",
        "  for l in ('0bk','2bk'):\n",
        "    timeseries_task_wm[l] = []\n",
        "\n",
        "  for subject in range(N_SUBJECTS):\n",
        "    if subject in drop:\n",
        "      continue\n",
        "\n",
        "    # Load full timeseries\n",
        "    timeseries = load_timeseries(subject=subject, name=task) # this is a 360, 810 numpy.ndarray\n",
        "    # Extract 2back timepoints \n",
        "    for l in ('0bk','2bk'):\n",
        "      evs = [load_evs(subject, task, cond) for cond in conds[l]]\n",
        "      ## Notes: evs is a list of 4 (sub)lists - 1 per condition\n",
        "      # Each sublist contains 2 dictionaries - 1 per run\n",
        "      # Each dictionary contains onset, duration, and amplitude keys\n",
        "\n",
        "      ts_block = np.zeros((N_PARCELS, N_CONDS*N_RUNS_TASK*BLOCK_FRAMES))\n",
        "      for i in range(N_CONDS):\n",
        "        for j in range(N_RUNS_TASK):\n",
        "          onset_frames = int(evs[i][j][\"onset\"]/TR) + j*FRAMES_PER_RUN\n",
        "          ts_block[:,(i*N_RUNS_TASK+j)*BLOCK_FRAMES:(i*N_RUNS_TASK+j + 1)*BLOCK_FRAMES] = timeseries[:,onset_frames:onset_frames+BLOCK_FRAMES]\n",
        "\n",
        "      # Concat new timeseries into timeseries_task_wm_2back\n",
        "\n",
        "      timeseries_task_wm[l].append(ts_block)\n",
        "\n",
        "  for l in timeseries_task_wm:\n",
        "    timeseries_task_wm[l] = mod.array(timeseries_task_wm[l])\n",
        "  return timeseries_task_wm\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmaVdZRDX4jS"
      },
      "source": [
        "#@title Checkpointing\n",
        "#@markdown Use the methods here to help create npy files to make checkpoints during pipelines\n",
        "\n",
        "class Checkpoint:\n",
        "  checkpoint_folder_name = \".checkpoints\"\n",
        "  def __init__(self):\n",
        "    self.folder = Path(self.checkpoint_folder_name)\n",
        "    if not self.folder.is_dir():\n",
        "      self.folder.mkdir()\n",
        "\n",
        "  def _file_path(self, name):\n",
        "    return (self.folder / name).with_suffix('.npy')\n",
        "  \n",
        "  def checkpoint_exists(self, name):\n",
        "    return self._file_path(name).is_file()\n",
        "\n",
        "  def save_checkpoint(self, name, data):\n",
        "    np.save(self._file_path(name), data)\n",
        "\n",
        "  def load_checkpoint(self, name):\n",
        "    if self.checkpoint_exists(name):\n",
        "      return np.load(self._file_path(name), allow_pickle=True)\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def remove_checkpoint(self, name):\n",
        "    p = self._file_path(name)\n",
        "    if p.is_file():\n",
        "      p.unlink()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQzCA99sMryW"
      },
      "source": [
        "## Task-based analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgnEuN0gMqxP"
      },
      "source": [
        "def condition_frames(run_evs, skip=0):\n",
        "  \"\"\"Identify timepoints corresponding to a given condition in each run.\n",
        "\n",
        "  Args:\n",
        "    run_evs (list of dicts) : Onset and duration of the event, per run\n",
        "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
        "      for hemodynamic lag\n",
        "\n",
        "  Returns:\n",
        "    frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
        "\n",
        "  \"\"\"\n",
        "  frames_list = []\n",
        "  for ev in run_evs:\n",
        "\n",
        "    # Determine when trial starts, rounded down\n",
        "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
        "\n",
        "    # Use trial duration to determine how many frames to include for trial\n",
        "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
        "\n",
        "    # Take the range of frames that correspond to this specific trial\n",
        "    frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
        "\n",
        "    frames_list.append(np.concatenate(frames))\n",
        "\n",
        "  return frames_list\n",
        "\n",
        "\n",
        "def selective_average(timeseries_data, ev, skip=0):\n",
        "  \"\"\"Take the temporal mean across frames for a given condition.\n",
        "\n",
        "  Args:\n",
        "    timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n",
        "    ev (dict or list of dicts): Condition timing information\n",
        "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
        "      for hemodynamic lag\n",
        "\n",
        "  Returns:\n",
        "    avg_data (1D array): Data averagted across selected image frames based\n",
        "    on condition timing\n",
        "\n",
        "  \"\"\"\n",
        "  # Ensure that we have lists of the same length\n",
        "  if not isinstance(timeseries_data, list):\n",
        "    timeseries_data = [timeseries_data]\n",
        "  if not isinstance(ev, list):\n",
        "    ev = [ev]\n",
        "  if len(timeseries_data) != len(ev):\n",
        "    raise ValueError(\"Length of `timeseries_data` and `ev` must match.\")\n",
        "\n",
        "  # Identify the indices of relevant frames\n",
        "  frames = condition_frames(ev, skip)\n",
        "\n",
        "  # Select the frames from each image\n",
        "  selected_data = []\n",
        "  for run_data, run_frames in zip(timeseries_data, frames):\n",
        "    run_frames = run_frames[run_frames < run_data.shape[1]]\n",
        "    selected_data.append(run_data[:, run_frames])\n",
        "\n",
        "  # Take the average in each parcel\n",
        "  avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)\n",
        "\n",
        "  return avg_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKOzUn2iPnQL"
      },
      "source": [
        "# Task analyses\n",
        "\n",
        "Description of each task, task timing, and conditions is located [here](https://protocols.humanconnectome.org/HCP/3T/task-fMRI-protocol-details.html).\n",
        "\n",
        "These are the condition names for each task:\n",
        "\n",
        "```\n",
        "- MOTOR: cue, lf, lh, rf, rh, t\n",
        "- WM:\n",
        "    0bk_body, 0bk_faces, 0bk_nir, 0bk_placed, 0bk_tools, \n",
        "    2bk_body, 2bk_faces, 2bk_nir, 2bk_placed, 2bk_tools,\n",
        "    0bk_cor, 0bk_err,\n",
        "    2bk_cor, 2bk_err,\n",
        "    all_bk_cor, all_bk_err\n",
        "- EMOTION: feat, neutral\n",
        "- GAMBLING: loss, loss_event, win, win_event, neut_event\n",
        "- LANGUAGE:\n",
        "    cue,\n",
        "    math, story\n",
        "    present_math, present_story,\n",
        "    question_math, question_story,\n",
        "    response_math, response_story\n",
        "- RELATIONAL: error, match, relation\n",
        "- SOCIAL: mental_resp, mental, other_resp, rnd\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlFhngy2UkTq"
      },
      "source": [
        "## Load individual runs for a given task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6_wbj3tM4wE"
      },
      "source": [
        "print(os.listdir(f\"{HCP_DIR}/subjects/128/EVs/tfMRI_WM_RL/\"))\n",
        "!cat ./hcp/subjects/128/EVs/tfMRI_WM_RL/2bk_body.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-oxgXHKn8v4"
      },
      "source": [
        "Load working memory data for each subject"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaYkLCfcn8Oq"
      },
      "source": [
        "task = \"wm\"\n",
        "conds = {'0bk': [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\"],\n",
        "         '2bk': [\"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\"]}\n",
        "\n",
        "N_CONDS = len(conds['2bk'])\n",
        "BLOCK_FRAMES = int(27.5/TR) # 27.5s is the duration of each block\n",
        "\n",
        "# intialize\n",
        "timeseries_task_wm = {}\n",
        "for l in ('0bk','2bk'):\n",
        "  timeseries_task_wm[l] = []\n",
        "\n",
        "for subject in range(N_SUBJECTS):\n",
        "\n",
        "  # Load full timeseries\n",
        "  timeseries = load_timeseries(subject=subject, name=task, remove_mean=False) # this is a 360, 810 numpy.ndarray\n",
        "  # Extract 2back timepoints \n",
        "  for l in ('0bk','2bk'):\n",
        "    evs = [load_evs(subject, task, cond) for cond in conds[l]]\n",
        "    ## Notes: evs is a list of 4 (sub)lists - 1 per condition\n",
        "    # Each sublist contains 2 dictionaries - 1 per run\n",
        "    # Each dictionary contains onset, duration, and amplitude keys\n",
        "\n",
        "    ts_block = np.zeros((N_PARCELS, N_CONDS*N_RUNS_TASK*BLOCK_FRAMES))\n",
        "    for i in range(N_CONDS):\n",
        "      for j in range(N_RUNS_TASK):\n",
        "        onset_frames = int(evs[i][j][\"onset\"]/TR) + j*FRAMES_PER_RUN\n",
        "        ts_block[:,(i*N_RUNS_TASK+j)*BLOCK_FRAMES:(i*N_RUNS_TASK+j + 1)*BLOCK_FRAMES] = timeseries[:,onset_frames:onset_frames+BLOCK_FRAMES]\n",
        "\n",
        "    # Concat new timeseries into timeseries_task_wm_2back\n",
        "\n",
        "    timeseries_task_wm[l].append(ts_block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6XQc4JDOlEA"
      },
      "source": [
        "## Run a simple correlation-based \"functional connectivity\" analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpQgzqh4MuoL"
      },
      "source": [
        "Generate a correlation matrix (showing \"functional connectivity\" or FC) for each subject and plot the group average:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_OdmN_wyDis"
      },
      "source": [
        "fc_2bk = np.zeros((N_SUBJECTS, N_PARCELS, N_PARCELS))\n",
        "fc_0bk = np.zeros((N_SUBJECTS, N_PARCELS, N_PARCELS))\n",
        "for sub, ts in enumerate(timeseries_task_wm['2bk']):\n",
        "  fc_2bk[sub] = np.corrcoef(ts)\n",
        "for sub, ts in enumerate(timeseries_task_wm['0bk']):\n",
        "  fc_0bk[sub] = np.corrcoef(ts)\n",
        "\n",
        "\n",
        "group_fc_2bk = fc_2bk.mean(axis=0)\n",
        "\n",
        "plt.imshow(group_fc_2bk, interpolation=\"none\", cmap=\"bwr\", vmin=-1, vmax=1)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "group_fc_0bk = fc_0bk.mean(axis=0)\n",
        "plt.imshow(group_fc_0bk, interpolation=\"none\", cmap=\"bwr\", vmin=-1, vmax=1)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUBV9CGLNkZ9"
      },
      "source": [
        "Threshold the correlation matrix to produce a connectome, and plot it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4d_qhh1Npd-"
      },
      "source": [
        "plotting.view_connectome(group_fc_2bk, atlas[\"coords\"], edge_threshold=\"99%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n62NhqlitGoM"
      },
      "source": [
        "## Extract connectivity features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03UvWa67EM7n"
      },
      "source": [
        "Function to calculate the node-wise functional connectivity summary features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmNoP8PrEKmy"
      },
      "source": [
        "#returns matrix correlations between seed and target with dimensions (subjects, seed_rois, target_rois)\n",
        "def extract_connectivities(seed_rois, target_rois, fc):\n",
        "  fc_summary = np.zeros((len(fc),len(seed_rois),len(target_rois)))\n",
        "  for i_seed in range(len(seed_rois)):\n",
        "    seed_idx = region_info[\"name\"].index(seed_rois[i_seed])\n",
        "    for i_target in range(len(target_rois)):\n",
        "      target_idx = region_info[\"name\"].index(target_rois[i_target])\n",
        "      if target_idx == seed_idx:\n",
        "        continue \n",
        "      for i_subj in range(len(fc)):\n",
        "        fc_summary[i_subj,i_seed,i_target] = fc_2bk[i_subj,seed_idx,target_idx]\n",
        "\n",
        "  return fc_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_EPnE0t5piR"
      },
      "source": [
        "# Node-to-node connectivity\n",
        "# Structure: array [nSubj x nNode x nNode]\n",
        "nodes = [\"R_p9-46v\", \"R_IP2\", \"R_7Pm\", \"R_AVI\", \"L_a9-46v\", \"L_46\", \"L_AIP\", \"L_MI\"]\n",
        "nNodes = len(nodes)\n",
        "fc2bk_node2node = extract_connectivities(nodes, nodes, fc_2bk)\n",
        "fc0bk_node2node = extract_connectivities(nodes, nodes, fc_0bk)\n",
        "\n",
        "# Node-to-within-netowrk connectivity\n",
        "# Structure: list [nNode], each entry is a vector [nSubj]\n",
        "fc2bk_node2wnet = {}; fc0bk_node2wnet = {}\n",
        "for node in nodes:\n",
        "  net = region_info['network'][region_info['name'].index(node)]\n",
        "  fc2bk_node2wnet[node] = np.mean(extract_connectivities([node], network_regions[net], fc_2bk), axis=2)\n",
        "  fc0bk_node2wnet[node] = np.mean(extract_connectivities([node], network_regions[net], fc_0bk), axis=2)\n",
        "\n",
        "# Node-to-other-netowrk connectivity\n",
        "# Structure: list [nNode], each entry is a list [nNet <minus self>], each list is a vector [nSubj]\n",
        "networks = ['Cingulo-Oper', 'Default', 'Dorsal-atten', 'Frontopariet']\n",
        "nNets = len(networks)\n",
        "fc2bk_node2onet = {}; fc0bk_node2onet = {}\n",
        "for node in nodes:\n",
        "  fc2bk_node2onet[node] = {}; fc0bk_node2onet[node] = {}\n",
        "  for net in networks:\n",
        "    # skip if this has already been calculated for within network connectivity\n",
        "    if net == region_info['network'][region_info['name'].index(node)]:\n",
        "      continue\n",
        "    fc2bk_node2onet[node][net] = np.mean(extract_connectivities([node], network_regions[net], fc_2bk), axis=2)\n",
        "    fc0bk_node2onet[node][net] = np.mean(extract_connectivities([node], network_regions[net], fc_0bk), axis=2)\n",
        "\n",
        "# Node-to-whole-brain connectivity\n",
        "# Structure: array [nSubj x nNode]\n",
        "fc2bk_node2brain = np.mean(extract_connectivities(nodes, region_info['name'], fc_2bk), axis=2)\n",
        "fc0bk_node2brain = np.mean(extract_connectivities(nodes, region_info['name'], fc_0bk), axis=2)\n",
        "\n",
        "# Between-network and network-to-brain connectivity\n",
        "# Structure: array [nSubj x nNet x nNet]\n",
        "fc2bk_net2net = np.zeros((N_SUBJECTS,len(networks),len(networks)))\n",
        "fc0bk_net2net = np.zeros((N_SUBJECTS,len(networks),len(networks)))\n",
        "# Structure: list [nNet], each entry is a vector [nSubj]\n",
        "fc2bk_net2brain = {}; fc0bk_net2brain = {}\n",
        "for i, net1 in enumerate(networks):\n",
        "  for j, net2 in enumerate(networks):\n",
        "    fc2bk_net2net[:,i,j] = np.mean(np.mean(extract_connectivities(network_regions[net1], network_regions[net2], fc_2bk), axis=2), axis=1)\n",
        "    fc0bk_net2net[:,i,j] = np.mean(np.mean(extract_connectivities(network_regions[net1], network_regions[net2], fc_0bk), axis=2), axis=1)\n",
        "  fc2bk_net2brain[net1] = np.mean(np.mean(extract_connectivities(network_regions[net1], region_info['name'], fc_2bk), axis=2), axis=1)\n",
        "  fc0bk_net2brain[net1] = np.mean(np.mean(extract_connectivities(network_regions[net1], region_info['name'], fc_0bk), axis=2), axis=1)\n",
        "\n",
        "# Adjust for missing data\n",
        "def adjust_fc_missing(fc0bk_node2node, fc0bk_node2wnet, fc0bk_node2onet,\n",
        "                      fc0bk_node2brain, fc0bk_net2net, fc0bk_net2brain,\n",
        "                      fc2bk_node2node, fc2bk_node2wnet, fc2bk_node2onet,\n",
        "                      fc2bk_node2brain, fc2bk_net2net, fc2bk_net2brain,\n",
        "                      missing_subjs, nodes, networks):\n",
        "  fc2bk_node2node = np.delete(fc2bk_node2node, missing_subjs, axis=0)\n",
        "  for node in nodes:\n",
        "    fc2bk_node2wnet[node] = np.delete(fc2bk_node2wnet[node], missing_subjs)\n",
        "    for net in list(fc2bk_node2onet[node].keys()):\n",
        "      fc2bk_node2onet[node][net] = np.delete(fc2bk_node2onet[node][net], missing_subjs)\n",
        "  fc2bk_node2brain = np.delete(fc2bk_node2brain, missing_subjs, axis=0)\n",
        "  fc2bk_net2net = np.delete(fc2bk_net2net, missing_subjs, axis=0)\n",
        "  for net in networks:\n",
        "    fc2bk_net2brain[net] = np.delete(fc2bk_net2brain[net], missing_subjs)\n",
        "\n",
        "  fc0bk_node2node = np.delete(fc0bk_node2node, missing_subjs, axis=0)\n",
        "  for node in nodes:\n",
        "    fc0bk_node2wnet[node] = np.delete(fc0bk_node2wnet[node], missing_subjs)\n",
        "    for net in list(fc0bk_node2onet[node].keys()):\n",
        "      fc0bk_node2onet[node][net] = np.delete(fc0bk_node2onet[node][net], missing_subjs)\n",
        "  fc0bk_node2brain = np.delete(fc0bk_node2brain, missing_subjs, axis=0)\n",
        "  fc0bk_net2net = np.delete(fc0bk_net2net, missing_subjs, axis=0)\n",
        "  for net in networks:\n",
        "    fc0bk_net2brain[net] = np.delete(fc0bk_net2brain[net], missing_subjs)\n",
        "  \n",
        "  return fc0bk_node2node, fc0bk_node2wnet, fc0bk_node2onet, fc0bk_node2brain, fc0bk_net2net, fc0bk_net2brain, fc2bk_node2node, fc2bk_node2wnet, fc2bk_node2onet, fc2bk_node2brain, fc2bk_net2net, fc2bk_net2brain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-3L7hYwdaM1J"
      },
      "source": [
        "#@title Graph Thresholding\n",
        "#@markdown Run this to get the thresholding functions! Main entrypoint is below, \n",
        "#@markdown use `help(graph_threshold)` for full description\n",
        "\n",
        "#@markdown `graph_threshold(input, numSurr)`\n",
        "#@markdown\n",
        "#@markdown Be sure to change the runtime to gpu to speed this up as much as possible! Pass in cupy arrays rather than numpy arrays.\n",
        "def graph_threshold(input, numSurr):\n",
        "  \"\"\"\n",
        "    Thresholds FC data using fourier transform surrogates. Pass in numpy or cupy \n",
        "    array. Array must be at least 2 dimensions: the second to last must be nodes\n",
        "    (e.g. atlas ROI), and the last must be the timecourse. The first\n",
        "    dimension(s) may be any sort of indexing (e.g. subjects, conditions, etc). \n",
        "\n",
        "    Note that using cupy arrays is more than 10 times as fast as using numpy \n",
        "    arrays. Be sure to enable GPU in colab.\n",
        "\n",
        "    Returns array of same type as input array (numpy vs cupy). Shape will \n",
        "    be the same as the input, with the exception of the last two dimensions,\n",
        "    which will contain the thresholded functional connectivity graphs\n",
        "\n",
        "    Args:\n",
        "    x (numpy/cupy array of floats with at least 2 dims): data to be thresholded\n",
        "    numSurr (scalar): Number of surrogates to calculate\n",
        "\n",
        "    Returns:\n",
        "    (numpy/cupy array of floats) : Functional connectivity graphs \n",
        "  \"\"\"\n",
        "  xp = get_array_mod(input)\n",
        "  index_dims = input.shape[:-2]\n",
        "  if len(index_dims) > 1:\n",
        "    # We put the single number inside a tuple\n",
        "    index_dims = ( func.reduce(op.mul, input.shape[:-2]) )\n",
        "  elif len(index_dims) == 0:\n",
        "    index_dims = ( 1, )\n",
        "  # Here, if index_dims is an empty tuple, it will not contribute anything to\n",
        "  # the shape (i.e. working_shape will be a 2d tuple)\n",
        "  shape = (*index_dims, *input.shape[-2:])\n",
        "  working_input = xp.reshape(input, shape)\n",
        "  results = xp.empty((*index_dims, *corrcoef_shape(input)))\n",
        "  for i, sample in enumerate(working_input):\n",
        "    results[i] = surrogate_threshold(sample, numSurr)\n",
        "  \n",
        "  return xp.reshape(results, (*input.shape[:-2], *corrcoef_shape(input)))\n",
        "\n",
        "\n",
        "def surrogate_threshold(input, numSurr):\n",
        "  xp = get_array_mod(input)\n",
        "  assert(len(input.shape) == 2)\n",
        "  fc_surr = xp.empty((numSurr, *corrcoef_shape(input)))\n",
        "  for i in range(numSurr):\n",
        "    surr = phase_randomize(input)\n",
        "    fc_surr[i] = xp.corrcoef(surr)\n",
        "\n",
        "  fc = xp.corrcoef(input)\n",
        "  cp.cuda.Device().synchronize()\n",
        "\n",
        "  if xp is cp:\n",
        "    fc = cp.asnumpy(fc)\n",
        "    fc_surr = cp.asnumpy(fc_surr)\n",
        "  pvals = sc.stats.mstats.ttest_1samp(fc_surr, fc, axis=0).pvalue\n",
        "  reject = two_d_multipletest(pvals)\n",
        "  return reject\n",
        "  fc[~reject] = 0\n",
        "  if xp is cp:\n",
        "    fc = cp.asarray(fc)\n",
        "  return fc\n",
        "\n",
        "\n",
        "def two_d_multipletest(pvals, method='bonferroni'):\n",
        "  reject = sm.stats.multipletests(np.reshape(pvals, -1), method=method)[0]\n",
        "  return np.reshape(reject, pvals.shape)\n",
        "\n",
        "\n",
        "def phase_randomize(input):\n",
        "  xp = get_array_mod(input)\n",
        "  f_len = input.shape[-1]\n",
        "  # 1. Calculate the Fourier transform of the original signal.\n",
        "  f_transform = xp.fft.fft(input, f_len, axis=-1)\n",
        "  amplitudes = np.abs(f_transform)\n",
        "  # 2. Generate a vector of random phases (i.e. a random sequence of values in\n",
        "  #    the range [0, 2pi]) , with length L/2 , where L is the length of the time\n",
        "  #    series.\n",
        "  #    In this implementation, we make phases the same length as the transform,\n",
        "  #    then symmetrize the phases by setting the front half as equal to the \n",
        "  #    negative of the back half.\n",
        "  phases = xp.random.uniform(-xp.pi, xp.pi, input.shape)\n",
        "  phases[..., f_len//2:] = -phases[..., f_len//2:0:-1]\n",
        "  phases[..., 0] = 0\n",
        "\n",
        "  # 3. As the Fourier transform is symmetrical, to create the new phase \n",
        "  #    randomized vector, multiply the first half of F (i.e. the half\n",
        "  #    corresponding to the positive frequencies) by the phases to create the\n",
        "  #    first half of F_r. The remainder of F_r is then the horizontally flipped\n",
        "  #    complex conjugate of the first half. \n",
        "\n",
        "  phases_added = amplitudes * xp.exp(1j * phases)\n",
        "\n",
        "\n",
        "  # 4. Finally, the inverse Fourier transform F_r of gives the FT surrogate. \n",
        "  #    Specifying time_len (the length of our original matrix) automatically\n",
        "  #    pads the input array with 2 0s in each trial (as a result of the above\n",
        "  #    operations, the array has 2 fewer timepoints than our input)\n",
        "  return xp.real(xp.fft.ifft(phases_added, f_len))\n",
        "\n",
        "def corrcoef_shape(a):\n",
        "  assert(len(a.shape) > 1)\n",
        "  return (a.shape[-2], a.shape[-2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ospkfRT4Uj0C"
      },
      "source": [
        "wm = load_wm_data(cp)[\"2bk\"]\n",
        "reject = surrogate_threshold(wm[2], 500)\n",
        "\n",
        "fc_matrix = pd.DataFrame(reject)\n",
        "# if use_node_names:\n",
        "#   fc_matrix.columns = region_info[\"name\"]\n",
        "#   fc_matrix.index = region_info[\"name\"]\n",
        "#   fc_matrix = fc_matrix.sort_index(0).sort_index(0)\n",
        "\n",
        "plt.imshow(fc_matrix, interpolation=\"none\", cmap=\"bwr\", vmin=-1, vmax=1)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epKJIeJfyZqH"
      },
      "source": [
        "### Graph Theory\n",
        "\n",
        "This is a work in progress :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7AsgNeX3IvQ"
      },
      "source": [
        "bins = np.arange(np.sqrt(len(np.concatenate(group_fc))))\n",
        "bins = (bins - np.min(bins))/np.ptp(bins)\n",
        "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
        "\n",
        "rawdist = sns.distplot(group_fc_nan.flatten(), bins=bins, kde=False, ax=axes[0], norm_hist=True)\n",
        "rawdist.set(xlabel='Correlation Values', ylabel = 'Density Frequency')\n",
        "\n",
        "log10dist = sns.distplot(np.log10(group_fc_nan).flatten(), kde=False, ax=axes[1], norm_hist=True)\n",
        "log10dist.set(xlabel='log(weights)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFS04-5sQDyz"
      },
      "source": [
        "#@title Functional Connectivity\n",
        "#@markdown Defines the class `FC_graph` for graph theory analysis\n",
        "class FC_graph:\n",
        "  \"\"\"\n",
        "    Class to produce functional connectivity graphs and derive metrics. Takes\n",
        "    functional covariance matrix as input, along with the region names. \n",
        "    Absolutizes the connectivity strength before creating the graph.\n",
        "  \"\"\"\n",
        "  # Nodal properties\n",
        "  WEIGHT = \"weight\"\n",
        "  STRENGTH = \"strength\"\n",
        "  STRENGTHNORM = \"strengthnorm\"\n",
        "  DISTANCE = \"distance\"\n",
        "  CLOSENESS = \"closeness\"\n",
        "  BETWEENNESS = \"bw\"\n",
        "  PATHLENGTH = \"pathlength\"\n",
        "  CLUSTERING = \"clustering\"\n",
        "  \n",
        "  def __init__(self, np_matrix, names):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        np_matrix (array[n,n]) : Functional covariance matrix\n",
        "        names (list[n]) : Names of each of the nodes in the covariance matrix\n",
        "    \"\"\"\n",
        "    self.data = abs(np_matrix)\n",
        "    self.G = self._prepare_graph(np_matrix, region_info[\"name\"])\n",
        "    self.names = names\n",
        "    self._compute_distance(self.G)\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "      Returns number of nodes in graph\n",
        "    \"\"\"\n",
        "    return nx.number_of_nodes(self.G)\n",
        "\n",
        "  def _prepare_graph(self, np_matrix, names):\n",
        "    G = nx.from_numpy_matrix(np_matrix)\n",
        "    G.remove_edges_from(list(nx.selfloop_edges(G)))\n",
        "    return nx.relabel_nodes(G, dict(enumerate(names)))\n",
        "\n",
        "  def _compute_distance(self, G):\n",
        "    # The function accepts a argument 'distance' that, in correlation-based \n",
        "    # networks, must be seen as the inverse of the weight value. Thus, a high\n",
        "    # correlation value (e.g., 0.8) means a shorter distance (i.e., 0.2).\n",
        "    G_distance_dict = {(e1, e2): 1 / abs(weight) for e1, e2, weight in G.edges(data=self.WEIGHT)}\n",
        "\n",
        "    # Then add them as attributes to the graph edges\n",
        "    nx.set_edge_attributes(self.G, G_distance_dict, self.DISTANCE)\n",
        "\n",
        "  def _compute_degree(self, G, node):\n",
        "    if not nx.get_node_attributes(G, self.STRENGTH):\n",
        "      strength = G.degree(weight=self.WEIGHT)\n",
        "      strengths = dict(strength)\n",
        "      nx.set_node_attributes(G, strengths, self.STRENGTH) # Add as nodal attribute\n",
        "\n",
        "    if not nx.get_node_attributes(G, self.STRENGTHNORM):\n",
        "      # Normalized node strength values 1/N-1\n",
        "      normstrenghts = {node: val * 1/(len(G.nodes)-1) for (node, val) in strength}\n",
        "      nx.set_node_attributes(G, normstrenghts, self.STRENGTHNORM)\n",
        "    return nx.get_node_attributes(G, self.STRENGTHNORM)[node]\n",
        "\n",
        "  def _compute_betweeness_centrality(self, G, node):\n",
        "    betweenness = nx.get_node_attributes(G, self.BETWEENNESS)\n",
        "    if betweenness and node in betweeness:\n",
        "      return betweenness[node]\n",
        "    betweenness = nx.betweenness_centrality(G, weight='distance', normalized=True) \n",
        "    nx.set_node_attributes(G, betweenness, 'bc')\n",
        "    return betweenness[node]\n",
        "\n",
        "  def _compute_shortest_path(self, G, node):\n",
        "    path = nx.get_node_attributes(G, self.PATHLENGTH)\n",
        "    if path and node in path:\n",
        "      return path[node]\n",
        "    path_lengths = nx.shortest_path_length(G, source=node, weight='distance')\n",
        "    path_dict = { node: np.mean(list(path_lengths.values())) }\n",
        "    #nx.set_node_attributes(G, path_dict, self.PATHLENGTH)\n",
        "    return path_dict[node]\n",
        "\n",
        "  def _compute_clustering(self, G, node):\n",
        "    clusters = nx.get_node_attributes(G, self.CLUSTERING)\n",
        "    if clusters and node in clusters:\n",
        "      return clusters[node]\n",
        "    clusters = nx.clustering(G, weight=self.DISTANCE, nodes=[node])\n",
        "    #nx.set_node_attributes(G, clusters, self.CLUSTERING)\n",
        "    return clusters[node]\n",
        "\n",
        "\n",
        "  def _node_subnet(self, node):\n",
        "    return dict(zip(self.names, self.networks))[node]\n",
        "\n",
        "  def _make_subnet(self, data, names, subnet_names):\n",
        "    i = np.array([name in subnet_names for name in names]).nonzero()[0]\n",
        "    i_1 = np.reshape(i, (1, i.shape[0]))\n",
        "    i_1 = np.repeat(i_1, i_1.shape[1], axis=0)\n",
        "    i_2 = np.reshape(i, (i.shape[0], 1))\n",
        "    i_2 = np.repeat(i_2, i_2.shape[0], axis=1)\n",
        "    \n",
        "    return FC_graph(data[i_1, i_2], subnet_names)\n",
        "\n",
        "\n",
        "  # Methods to produce derivative graphs\n",
        "  def get_subgraphs(self, region_info):\n",
        "    \"\"\"\n",
        "      Returns a dict of new instances of FC_graph, each entry containing the \n",
        "      graph for the network of interest.\n",
        "\n",
        "      Args:\n",
        "        region_info (dict) : the region info dict derived from previous code blocks\n",
        "\n",
        "      Returns:\n",
        "        dict(network: FC_graph) : the subgraph containing the network of interest\n",
        "    \"\"\"\n",
        "    network_members = defaultdict(list)\n",
        "    names = region_info[\"name\"]\n",
        "    for net, name in zip(region_info[\"network\"], names):\n",
        "      network_members[net].append(name)\n",
        "\n",
        "    return { network: self._make_subnet(self.data, names, members)\n",
        "                     for network, members in network_members.items() }\n",
        "\n",
        "  def get_sparser_graph(self, threshold):\n",
        "    \"\"\"\n",
        "      Thresholds the connectivity based on an absolute threshold and returns\n",
        "      a new graph\n",
        "\n",
        "      Args:\n",
        "        threshold (int) : Number in range [0, 1]\n",
        "      \n",
        "      Returns:\n",
        "        FC_graph\n",
        "    \"\"\"\n",
        "    new_matrix = self.data.copy()\n",
        "    new_matrix[new_matrix<=threshold] = 0\n",
        "    return FC_graph(new_matrix, self.region_info)\n",
        "\n",
        "\n",
        "  # Metric methods\n",
        "\n",
        "  def hubness(self, nodes):\n",
        "    \"\"\"\n",
        "      Calculates four metrics that relate to the hubness of the given node:\n",
        "        - Degree: The sum of the node's connections strength to all other nodes\n",
        "        - Path: The average shortest path length between the node and all others\n",
        "        - Betweenness Centrality: The proportion of shortest paths between every\n",
        "            other node that pass through the node of interest\n",
        "        - Clustering cooefficient: The proportion of the node's neighbors that are\n",
        "            connected, weighted by their connection strength\n",
        "      \n",
        "      These metrics are returned as a dict:\n",
        "      ```\n",
        "      { node: np.array(degree, path length, betweenness centrality, clustering cooefficient) }\n",
        "      ```\n",
        "\n",
        "      Args:\n",
        "        nodes (string, list-like) : Either a single node or a list of\n",
        "          nodes to be evaluated\n",
        "\n",
        "      Returns:\n",
        "        dict(np.array) : The four hubness values\n",
        "    \"\"\"\n",
        "    if type(nodes) is str:\n",
        "      nodes = [nodes]\n",
        "    assert type(nodes) in (tuple, list, np.array), \"Nodes must be an iterable container\"\n",
        "    return { node: np.array([\n",
        "      self._compute_degree(self.G, node),\n",
        "      self._compute_shortest_path(self.G, node), \n",
        "      self._compute_betweeness_centrality(self.G, node), \n",
        "      self._compute_clustering(self.G, node)\n",
        "    ]) for node in nodes }\n",
        "\n",
        "  def mean_degree(self):\n",
        "    strengths = nx.get_node_attributes(self.G, self.STRENGTHNORM).values()\n",
        "    normstrengthlist = np.array(list(strengths))\n",
        "    return np.sum(normstrengthlist)/len(self.G.nodes)\n",
        "\n",
        "\n",
        "  # Plotting Functions\n",
        "\n",
        "  def plot_connectome(self, use_node_names=False ):\n",
        "    fc_matrix = pd.DataFrame(group_fc_nan)\n",
        "    if use_node_names:\n",
        "      fc_matrix.columns = region_info[\"name\"]\n",
        "      fc_matrix.index = region_info[\"name\"]\n",
        "      fc_matrix = fc_matrix.sort_index(0).sort_index(0)\n",
        "    \n",
        "    plt.imshow(fc_matrix, interpolation=\"none\", cmap=\"bwr\", vmin=-1, vmax=1)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "  def betweenness_centrality(self):\n",
        "    betweenness = self._compute_betweeness_centrality(self.G)\n",
        "    sns.distplot(list(betweenness.values()), kde=False)\n",
        "    plt.xlabel('Centrality Values')\n",
        "    plt.ylabel('Counts') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHWgEwc8zMQY"
      },
      "source": [
        "#@title Graph Theory Pipeline\n",
        "NUM_SURRUGATES = 500 \n",
        "MODULE = cp #cupy# #@param [\"cp #cupy#\", \"np #numpy#\"] {type:\"raw\"}\n",
        "NUM_SURRUGATES = 500 #@param {type:\"number\"}\n",
        "IGNORE_CHECKPOINTS = False #@param {type:\"boolean\"}\n",
        "THRESHOLD_CHECKPOINT = \"threshold\"\n",
        "NUM_CPUS = 16\n",
        "\n",
        "import time\n",
        "\n",
        "def get_graph_stats(subject):\n",
        "  return FC_graph(subject, region_info[\"name\"]).hubness(NODES_OF_INTEREST)\n",
        "\n",
        "\n",
        "def graph_theory_pipeline():\n",
        "  ch = Checkpoint()\n",
        "  if IGNORE_CHECKPOINTS:\n",
        "    ch.remove_checkpoint(THRESHOLD_CHECKPOINT)\n",
        "  if not ch.checkpoint_exists(THRESHOLD_CHECKPOINT):\n",
        "    data = load_wm_data(MODULE)\n",
        "    data_2bk = data['2bk']\n",
        "    fc = graph_threshold(data_2bk, NUM_SURRUGATES)\n",
        "    if MODULE is cp:\n",
        "      fc = cp.asnumpy(fc)\n",
        "    del data\n",
        "    del data_2bk\n",
        "    ch.save_checkpoint(THRESHOLD_CHECKPOINT, fc)\n",
        "  else:\n",
        "    fc = ch.load_checkpoint(THRESHOLD_CHECKPOINT)\n",
        "  if not ch.checkpoint_exists('hubness'):\n",
        "    pool = mult.Pool(NUM_CPUS)\n",
        "    start_time = time.process_time()\n",
        "    #hubness = get_graph_stats(fc[0])\n",
        "    hubness = np.array(list(pool.imap(get_graph_stats, fc, fc.shape[0]//NUM_CPUS)))\n",
        "    print(time.process_time() - start_time)\n",
        "    ch.save_checkpoint('hubness', hubness)\n",
        "  else:\n",
        "    hubness = ch.load_checkpoint('hubness')\n",
        "  return hubness\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkd1ZTGO0qVB"
      },
      "source": [
        "os.cpu_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmxNKtGFxmgx"
      },
      "source": [
        "# Behavioral covariates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgnYw0oxYOtB"
      },
      "source": [
        "## Task performance measures\n",
        "\n",
        "The dataset also includes aggregate behavior for each task run stored in task-specific `.csv` files. It is possible to load and work with these files using `numpy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PxLSdGYXfen"
      },
      "source": [
        "wm_behavior = np.genfromtxt(\"hcp/behavior/wm.csv\",\n",
        "                            delimiter=\",\",\n",
        "                            names=True,\n",
        "                            dtype=None,\n",
        "                            encoding=\"utf\")\n",
        "\n",
        "medianRT_0bk = np.zeros(N_SUBJECTS)\n",
        "medianRT_2bk = np.zeros(N_SUBJECTS)\n",
        "# Pull data manually to avoid confusion with missing subjects\n",
        "for i in range(N_SUBJECTS):\n",
        "  tmp0 = np.zeros(N_CONDS*N_RUNS_TASK)\n",
        "  tmp2 = np.zeros(N_CONDS*N_RUNS_TASK)\n",
        "  idx = 0\n",
        "  for k in (0,1):\n",
        "    for l in (\"BODY\",\"FACE\",\"TOOL\",\"PLACE\"):\n",
        "      cond0 = \"0BK_\"+l\n",
        "      cond2 = \"2BK_\"+l\n",
        "      idx0 = np.logical_and(wm_behavior['Subject'] == i, wm_behavior['Run'] == k)\n",
        "      idx2 = np.logical_and(idx0, wm_behavior['ConditionName'] == cond2)\n",
        "      idx0 = np.logical_and(idx0, wm_behavior['ConditionName'] == cond0)\n",
        "      if np.sum(idx0) > 0:\n",
        "        tmp0[idx] = wm_behavior['MEDIAN_RT'][np.where(idx0 == True)]\n",
        "      if np.sum(idx2) > 0:\n",
        "        tmp2[idx] = wm_behavior['MEDIAN_RT'][np.where(idx2 == True)]\n",
        "      idx = idx+1\n",
        "  if tmp0.min() > 0:\n",
        "    medianRT_0bk[i] = np.mean(tmp0)\n",
        "  else:\n",
        "    medianRT_0bk[i] = float('nan')\n",
        "  if tmp2.min() > 0:\n",
        "    medianRT_2bk[i] = np.mean(tmp2)\n",
        "  else:\n",
        "    medianRT_2bk[i] = float('nan')\n",
        "missing_subjs = np.where(np.logical_and(np.isnan(medianRT_0bk), np.isnan(medianRT_2bk)))\n",
        "missing_subjs = missing_subjs[0]\n",
        "N_SUBJ_ADJ = N_SUBJECTS-len(missing_subjs)\n",
        "medianRT_0bk = np.delete(medianRT_0bk, missing_subjs)\n",
        "medianRT_2bk = np.delete(medianRT_2bk, missing_subjs)\n",
        "if subj_adjust == False:\n",
        "  print('WARNINGS: FC DATA DELETED FOR MISSING SUBJECTS!')\n",
        "  fc0bk_node2node, fc0bk_node2wnet, fc0bk_node2onet, fc0bk_node2brain, fc0bk_net2net, fc0bk_net2brain, fc2bk_node2node, fc2bk_node2wnet, fc2bk_node2onet, fc2bk_node2brain, fc2bk_net2net, fc2bk_net2brain = adjust_fc_missing(fc0bk_node2node, fc0bk_node2wnet, fc0bk_node2onet,\n",
        "                      fc0bk_node2brain, fc0bk_net2net, fc0bk_net2brain,\n",
        "                      fc2bk_node2node, fc2bk_node2wnet, fc2bk_node2onet,\n",
        "                      fc2bk_node2brain, fc2bk_net2net, fc2bk_net2brain,\n",
        "                      missing_subjs, nodes, networks)\n",
        "  subj_adjust = True\n",
        "\n",
        "# Make separate dataframes for 0 and 2 back performance measures\n",
        "back0_array = (wm_behavior[np.where((wm_behavior[\"ConditionName\"] == \"0BK_BODY\") | (wm_behavior[\"ConditionName\"] == \"0BK_FACE\") | (wm_behavior[\"ConditionName\"] == \"0BK_TOOL\") | (wm_behavior[\"ConditionName\"] == \"0BK_PLACE\"))])\n",
        "back2_array = (wm_behavior[np.where((wm_behavior[\"ConditionName\"] == \"2BK_BODY\") | (wm_behavior[\"ConditionName\"] == \"2BK_FACE\") | (wm_behavior[\"ConditionName\"] == \"2BK_TOOL\") | (wm_behavior[\"ConditionName\"] == \"2BK_PLACE\"))])\n",
        "\n",
        "back0_df = pd.DataFrame(back0_array)\n",
        "back2_df = pd.DataFrame(back2_array)\n",
        "\n",
        "# Make dataframes with 1 performance score per subject (averaged across runs)\n",
        "back0_mean = back0_df.groupby('Subject').mean()\n",
        "back2_mean = back2_df.groupby('Subject').mean()\n",
        "back0_mean = back0_mean.drop(missing_subjs[0])\n",
        "\n",
        "# Concat 0 and 2 back dataframes\n",
        "wm_behavior_byBlock = pd.concat([back0_mean, back2_mean], keys=['0back', '2back'])\n",
        "\n",
        "# Average by subject and run\n",
        "#back0_df.groupby(['Subject', 'Run']).mean()\n",
        "#back2_df.groupby(['Subject', 'Run']).mean()\n",
        "\n",
        "# Accuracy Mean and standard deviation \n",
        "print(\"0 back Accuracy; mean SD\")\n",
        "print(round(back0_mean[\"ACC\"].mean(), 2), round(back0_mean[\"ACC\"].std(),2 ))\n",
        "print(\"2 back Accuracy; mean SD\")\n",
        "print(round(back2_mean[\"ACC\"].mean(),2), round(back2_mean[\"ACC\"].std(),2))\n",
        "\n",
        "# RT Mean and standard deviation \n",
        "print(\"0 back MEDIAN_RT; mean SD\")\n",
        "print(round(back0_mean[\"MEDIAN_RT\"].mean(), 2), round(back0_mean[\"MEDIAN_RT\"].std(),2 ))\n",
        "print(\"2 back MEDIAN_RT; mean SD\")\n",
        "print(round(back2_mean[\"MEDIAN_RT\"].mean(), 2), round(back2_mean[\"MEDIAN_RT\"].std(),2 ))\n",
        "\n",
        "# Histograms\n",
        "#back0_mean.hist(bins=20)\n",
        "#back2_mean.hist(bins=20)\n",
        "hist_acc_0 = back0_mean[\"ACC\"].hist(bins=20, color = \"blue\", alpha=0.3)\n",
        "hist_acc_2 = back2_mean[\"ACC\"].hist(bins=20, color = \"red\", alpha=0.3)\n",
        "\n",
        "hist_rt_0 = back0_mean[\"MEDIAN_RT\"].hist(bins=20, color = \"green\", alpha=0.3)\n",
        "hist_rt_2 = back2_mean[\"MEDIAN_RT\"].hist(bins=20, color = \"yellow\", alpha=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cHs2w31WHl1"
      },
      "source": [
        "But, while not formally taught as part of the course, [`pandas`](https://pandas.pydata.org/) offers more powerful tools for tabular data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRoHdRdagLq3"
      },
      "source": [
        "print(len(medianRT_2bk))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1XSv28bbYMh"
      },
      "source": [
        "## Linear Regrssion Models in Resting State Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONsRDGWrK2wV"
      },
      "source": [
        "y = medianRT_2bk\n",
        "y0 = medianRT_0bk\n",
        "#print(y)\n",
        "#print(y0)\n",
        "\n",
        "# Measures\n",
        "sse = np.zeros(6)\n",
        "acc = np.zeros(6)\n",
        "\n",
        "# Node-to-node models\n",
        "# Structure: array [nNode x nNode]\n",
        "mods_node2node = {}\n",
        "coef_node2node = np.zeros((nNodes,nNodes))\n",
        "x_tot = np.zeros((N_SUBJ_ADJ,int((nNodes*(nNodes-1))/2)+1))\n",
        "k = 0\n",
        "for i,node1 in enumerate(nodes):\n",
        "  mods_node2node[node1] = {}\n",
        "  for j,node2 in enumerate(nodes):\n",
        "    if j >= i:\n",
        "      continue\n",
        "    x_tot[:,k] = fc2bk_node2node[:,i,j]\n",
        "    x = x_tot[:,k].reshape(-1,1)\n",
        "    k = k+1\n",
        "    mods_node2node[node1][node2] = LinearRegression(normalize=True).fit(x, y)\n",
        "    coef_node2node[i,j] = mods_node2node[node1][node2].coef_\n",
        "x_tot[:,-1] = y0\n",
        "mod_node2node = LinearRegression(normalize=True).fit(x_tot, y)\n",
        "yhat = mod_node2node.predict(x_tot)\n",
        "sse[0] = np.sum((y-yhat)**2)\n",
        "mod_node2node = LinearRegression(normalize=True).fit(x_tot[0:int(N_SUBJ_ADJ/2),:], y[0:int(N_SUBJ_ADJ/2)])\n",
        "yhat = mod_node2node.predict(x_tot[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ,:])\n",
        "acc[0] = np.sum((y[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ]-yhat)**2)\n",
        "\n",
        "# Node-to-within-netowrk models\n",
        "# Structure: vector [nNode]\n",
        "mods_node2wnet = {}\n",
        "coef_node2wnet = np.zeros(nNodes)\n",
        "x_tot = np.zeros((N_SUBJ_ADJ,nNodes+1))\n",
        "k = 0\n",
        "for i, node in enumerate(nodes):\n",
        "  x_tot[:,k] = fc2bk_node2wnet[node]\n",
        "  x = x_tot[:,k].reshape(-1,1)\n",
        "  k = k+1\n",
        "  mods_node2wnet[node] = LinearRegression(normalize=True).fit(x, y)\n",
        "  coef_node2wnet[i] = mods_node2wnet[node].coef_\n",
        "x_tot[:,-1] = y0\n",
        "mod_node2wnet = LinearRegression(normalize=True).fit(x_tot, y)\n",
        "yhat = mod_node2wnet.predict(x_tot)\n",
        "sse[1] = np.sum((y-yhat)**2)\n",
        "mod_node2wnet = LinearRegression(normalize=True).fit(x_tot[0:int(N_SUBJ_ADJ/2),:], y[0:int(N_SUBJ_ADJ/2)])\n",
        "yhat = mod_node2wnet.predict(x_tot[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ,:])\n",
        "acc[1] = np.sum((y[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ]-yhat)**2)\n",
        "\n",
        "# Node-to-other-netowrk models\n",
        "# Structure: list [nNode], each entry is a list [nNet <minus self>]\n",
        "mods_node2onet = {}\n",
        "coef_node2onet = {}\n",
        "x_tot = np.zeros((N_SUBJ_ADJ,25+1))\n",
        "k = 0\n",
        "for node in nodes:\n",
        "  mods_node2onet[node] = {}; coef_node2onet[node] = {}\n",
        "  for net in list(fc2bk_node2onet[node].keys()):\n",
        "    x_tot[:,k] = fc2bk_node2onet[node][net]\n",
        "    x = x_tot[:,k].reshape(-1,1)\n",
        "    k = k+1\n",
        "    mods_node2onet[node][net] = LinearRegression(normalize=True).fit(x, y)\n",
        "    coef_node2onet[node][net] = mods_node2onet[node][net].coef_\n",
        "x_tot[:,-1] = y0\n",
        "mod_node2onet = LinearRegression(normalize=True).fit(x_tot, y)\n",
        "yhat = mod_node2onet.predict(x_tot)\n",
        "sse[2] = np.sum((y-yhat)**2)\n",
        "mod_node2onet = LinearRegression(normalize=True).fit(x_tot[0:int(N_SUBJ_ADJ/2),:], y[0:int(N_SUBJ_ADJ/2)])\n",
        "yhat = mod_node2onet.predict(x_tot[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ,:])\n",
        "acc[2] = np.sum((y[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ]-yhat)**2)\n",
        "\n",
        "# Node-to-whole-brain models\n",
        "# Structure: vector [nNode]\n",
        "mods_node2brain = {}\n",
        "coef_node2brain = np.zeros(nNodes)\n",
        "x_tot = np.zeros((N_SUBJ_ADJ,nNodes+1))\n",
        "k = 0\n",
        "for i, node in enumerate(nodes):\n",
        "  x_tot[:,k] = fc2bk_node2brain[:,i]\n",
        "  x = x_tot[:,k].reshape(-1,1)\n",
        "  k = k+1\n",
        "  mods_node2brain[node] = LinearRegression(normalize=True).fit(x, y)\n",
        "  coef_node2brain[i] = mods_node2brain[node].coef_\n",
        "x_tot[:,-1] = y0\n",
        "mod_node2brain = LinearRegression(normalize=True).fit(x_tot, y)\n",
        "yhat = mod_node2brain.predict(x_tot)\n",
        "sse[3] = np.sum((y-yhat)**2)\n",
        "mod_node2brain = LinearRegression(normalize=True).fit(x_tot[0:int(N_SUBJ_ADJ/2),:], y[0:int(N_SUBJ_ADJ/2)])\n",
        "yhat = mod_node2brain.predict(x_tot[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ,:])\n",
        "acc[3] = np.sum((y[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ]-yhat)**2)\n",
        "\n",
        "# Between-network models\n",
        "# Structure: array [nNet x nNet]\n",
        "mods_net2net = {}\n",
        "coef_net2net = np.zeros((nNets,nNets))\n",
        "x_tot = np.zeros((N_SUBJ_ADJ,int((nNets*(nNets-1))/2)+1))\n",
        "k = 0\n",
        "for i,net1 in enumerate(networks):\n",
        "  mods_net2net[net1] = {}\n",
        "  for j,net2 in enumerate(networks):\n",
        "    if j >= i:\n",
        "      continue\n",
        "    x_tot[:,k] = fc2bk_net2net[:,i,j]\n",
        "    x = x_tot[:,k].reshape(-1,1)\n",
        "    k = k+1\n",
        "    mods_net2net[net1][net2] = LinearRegression(normalize=True).fit(x, y)\n",
        "    coef_net2net[i,j] = mods_net2net[net1][net2].coef_\n",
        "x_tot[:,-1] = y0\n",
        "mod_net2net = LinearRegression(normalize=True).fit(x_tot,y)\n",
        "yhat = mod_net2net.predict(x_tot)\n",
        "sse[4] = np.sum((y-yhat)**2)\n",
        "mod_net2net = LinearRegression(normalize=True).fit(x_tot[0:int(N_SUBJ_ADJ/2),:], y[0:int(N_SUBJ_ADJ/2)])\n",
        "yhat = mod_net2net.predict(x_tot[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ,:])\n",
        "acc[4] = np.sum((y[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ]-yhat)**2)\n",
        "\n",
        "# Network-to-brain models\n",
        "# Structure: vector [nNet]\n",
        "mods_net2brain = {}\n",
        "coef_net2brain = np.zeros(nNet)\n",
        "x_tot = np.zeros((N_SUBJ_ADJ,nNets+1))\n",
        "k = 0\n",
        "for i,net in enumerate(networks):\n",
        "  x_tot[:,k] = fc2bk_net2brain[net]\n",
        "  x = x_tot[:,k].reshape(-1,1)\n",
        "  k = k+1;\n",
        "  mods_net2brain[net] = LinearRegression(normalize=True).fit(x, y)\n",
        "  coef_net2brain[i] = mods_net2brain[net].coef_\n",
        "x_tot[:,-1] = y0\n",
        "mod_net2brain = LinearRegression(normalize=True).fit(x_tot,y)\n",
        "yhat = mod_net2brain.predict(x_tot)\n",
        "sse[5] = np.sum((y-yhat)**2)\n",
        "mod_net2brain = LinearRegression(normalize=True).fit(x_tot[0:int(N_SUBJ_ADJ/2),:], y[0:int(N_SUBJ_ADJ/2)])\n",
        "yhat = mod_net2brain.predict(x_tot[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ,:])\n",
        "acc[5] = np.sum((y[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ]-yhat)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOLj6Y6dn7hB"
      },
      "source": [
        "print(sse)\n",
        "print(acc)\n",
        "scales = ['node2node','node2wnet','node2onet','node2brain','net2net','net2brain']\n",
        "x = np.arange(len(scales))\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(x+0.00, sse, color = 'b', width = 0.25)\n",
        "ax.bar(x+0.25, acc, color = 'r', width = 0.25)\n",
        "plt.xticks(x,scales)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkpoTt80artD"
      },
      "source": [
        "Attempt to run cross validation on Node-to-node model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fHwGvklfjwP"
      },
      "source": [
        "# Modify cross_validate function from tutorial 6\n",
        "# TO DO: add loop to function that will run LinearRegression() using a different subset of columns for each model\n",
        "# Once loop is \n",
        "\n",
        "n_models = 6 #use this as input to function?\n",
        "\n",
        "def cross_validate(x_train, y_train, n_splits, n_models):\n",
        "  \"\"\" Compute MSE for k-fold validation for each order polynomial\n",
        "\n",
        "  Args:\n",
        "    x_train (ndarray): training data input vector of shape (n_samples)\n",
        "    y_train (ndarray): training vector of measurements of shape (n_samples)\n",
        "    n_split (scalar): number of folds for k-fold validation\n",
        "    N_MODELS (scalar): number of models to run cross-validation on\n",
        "\n",
        "  Return:\n",
        "    ndarray: MSE over splits, shape (n_splits)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the split method\n",
        "  kfold_iterator = KFold(n_splits)\n",
        "\n",
        "  # Initialize np array mse values for all models for each split\n",
        "  mse_all = np.zeros((n_splits, N_MODELS))\n",
        "\n",
        "  for i_split, (train_indices, val_indices) in enumerate(kfold_iterator.split(x_train)):\n",
        "\n",
        "      # Split up the overall training data into cross-validation training and validation sets\n",
        "      x_cv_train = x_train[train_indices]\n",
        "      y_cv_train = y_train[train_indices]\n",
        "      x_cv_val = x_train[val_indices]\n",
        "      y_cv_val = y_train[val_indices]\n",
        "\n",
        "      # Loop through each model by selecting different predictors (i.e., columns of x_cv_train)\n",
        "      #for XXXX:   \n",
        "        ## Fit models on training folds\n",
        "        #model = LinearRegression(normalize=True).fit(x_cv_train[SUBSET], y_cv_train)\n",
        "        ## Predict on validation fold\n",
        "        #y_hat = model.predict(x_cv_val[SUBSET])\n",
        "\n",
        "        # Compute MSE\n",
        "        #residuals = y_cv_val - y_hat\n",
        "        #mse_this_split = np.mean(residuals ** 2)\n",
        "        #mse_all[i_split] = mse_this_split #MODIFY TO INDEX COLUMN FOR A SPECIFIC MODEL??\n",
        "\n",
        "      # Fit models on training folds\n",
        "      model = LinearRegression(normalize=True).fit(x_cv_train, y_cv_train)\n",
        "      # Predict on validation fold\n",
        "      y_hat = model.predict(x_cv_val)\n",
        "\n",
        "      # Compute MSE\n",
        "      residuals = y_cv_val - y_hat\n",
        "      mse_this_split = np.mean(residuals ** 2)\n",
        "      mse_all[i_split] = mse_this_split\n",
        "      print(type(mse_all))\n",
        "\n",
        "  return mse_all\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV8a4WVdjYcM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI40qx3saq7l"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "y = medianRT_2bk\n",
        "y0 = medianRT_0bk\n",
        "#print(y)\n",
        "#print(y0)\n",
        "\n",
        "# Measures\n",
        "sse = np.zeros(6)\n",
        "acc = np.zeros(6)\n",
        "\n",
        "# Node-to-within-netowrk models\n",
        "# Structure: vector [nNode]\n",
        "mods_node2wnet = {}\n",
        "coef_node2wnet = np.zeros(nNodes)\n",
        "x_tot = np.zeros((N_SUBJ_ADJ,nNodes+1))\n",
        "k = 0\n",
        "for i, node in enumerate(nodes):\n",
        "  x_tot[:,k] = fc2bk_node2wnet[node]\n",
        "  x = x_tot[:,k].reshape(-1,1)\n",
        "  k = k+1\n",
        "  mods_node2wnet[node] = LinearRegression(normalize=True).fit(x, y)\n",
        "  coef_node2wnet[i] = mods_node2wnet[node].coef_\n",
        "x_tot[:,-1] = y0\n",
        "mod_node2wnet = LinearRegression(normalize=True).fit(x_tot, y)\n",
        "yhat = mod_node2wnet.predict(x_tot)\n",
        "sse[1] = np.sum((y-yhat)**2)\n",
        "mod_node2wnet = LinearRegression(normalize=True).fit(x_tot[0:int(N_SUBJ_ADJ/2),:], y[0:int(N_SUBJ_ADJ/2)])\n",
        "yhat = mod_node2wnet.predict(x_tot[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ,:])\n",
        "acc[1] = np.sum((y[int(N_SUBJ_ADJ/2):N_SUBJ_ADJ]-yhat)**2)\n",
        "\n",
        "# create testing and training datasets\n",
        "## TO DO: put x's (x_tot) for each model into 1 big matrix, and run train_test_split on that\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_tot, y, test_size=0.2)\n",
        "#print(x_train.shape, y_train.shape)\n",
        "#print(x_test.shape, y_test.shape)\n",
        "\n",
        "# Cross-validate using training data\n",
        "n_splits = 10\n",
        "mse_model = cross_validate(x_train, y_train, n_splits, n_models=6)\n",
        "\n",
        "#Visualize mse\n",
        "#plt.boxplot(mse_model)\n",
        "\n",
        "# TO DO: Add mse values for each model (mse_model) to mse_all\n",
        "\n",
        "# To Do: Run cross_validate() on each model and extract mse_model from models\n",
        "# Note: x_train is a different set of data for each model -- number of columns is the number of predictors in the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH-PnTmWcwXa"
      },
      "source": [
        "Node to Network Adding 0bk behavior as Covariate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD2QDj2oZ8aI"
      },
      "source": [
        "#add 0bk rt as covar node to CO\n",
        "x1= rs_dlpfc_CO_all_mean\n",
        "back0_array_clean= back0_mean.to_numpy()\n",
        "x2=back0_array_clean[:,4]\n",
        "x3 = x2[np.logical_not(np.isnan(x2))].reshape((336,1))\n",
        "x= np.concatenate((x1, x3), axis=1)\n",
        "y= back2_mean['MEDIAN_RT'].to_numpy().reshape((336,1))\n",
        "model = LinearRegression().fit(x, y)\n",
        "print(model.coef_)\n",
        "plt.scatter(x[:,0],y) \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yqyAeHZaKSN"
      },
      "source": [
        "#FP\n",
        "x1= rs_dlpfc_FP_all_mean\n",
        "back0_array_clean= back0_mean.to_numpy()\n",
        "x2=back0_array_clean[:,4]\n",
        "x3 = x2[np.logical_not(np.isnan(x2))].reshape((336,1))\n",
        "x= np.concatenate((x1, x3), axis=1)\n",
        "y= back2_mean['MEDIAN_RT'].to_numpy().reshape((336,1))\n",
        "model = LinearRegression().fit(x, y)\n",
        "print(model.coef_)\n",
        "plt.scatter(x[:,0],y) \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YPcsquLekjV"
      },
      "source": [
        "#DAN\n",
        "x1= rs_dlpfc_DAN_all_mean\n",
        "back0_array_clean= back0_mean.to_numpy()\n",
        "x2=back0_array_clean[:,4]\n",
        "x3 = x2[np.logical_not(np.isnan(x2))].reshape((336,1))\n",
        "x= np.concatenate((x1, x3), axis=1)\n",
        "y= back2_mean['MEDIAN_RT'].to_numpy().reshape((336,1))\n",
        "model = LinearRegression().fit(x, y)\n",
        "print(model.coef_)\n",
        "plt.scatter(x[:,0],y) \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPt4uBFex7-"
      },
      "source": [
        "#DMN\n",
        "x1= rs_dlpfc_DMN_all_mean\n",
        "back0_array_clean= back0_mean.to_numpy()\n",
        "x2=back0_array_clean[:,4]\n",
        "x3 = x2[np.logical_not(np.isnan(x2))].reshape((336,1))\n",
        "x= np.concatenate((x1, x3), axis=1)\n",
        "y= back2_mean['MEDIAN_RT'].to_numpy().reshape((336,1))\n",
        "model = LinearRegression().fit(x, y)\n",
        "print(model.coef_)\n",
        "plt.scatter(x[:,0],y) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6UJ63g9WqFH"
      },
      "source": [
        "## Pseudo-demographics\n",
        "\n",
        "The NMA-distributed version of the HCP data does not contain any real demographic information. But we have created a synthetic dataset of 25 \"demographic\" variables based on a model trained on the original dataset to predict demographics from resting-state network organization measures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEbatTiEWvtC"
      },
      "source": [
        "demo = np.load(\"hcp/pseudo_demographics.npy\")\n",
        "demo.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HCgSp9nXB7r"
      },
      "source": [
        "## Original subject IDs\n",
        "\n",
        "The dataset also contains a key to map the ordinal subject numbers to the IDs used in the original HCP dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsd1CjXAXLB5"
      },
      "source": [
        "ids = np.loadtxt(\"hcp/orig_ids.txt\")\n",
        "print(ids[:8])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}